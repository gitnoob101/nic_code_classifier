{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b4947-b9e3-442f-9416-753bcbc3213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastapi uvicorn faiss-cpu torch transformers pandas numpy ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df32f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backend before update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29009af-ca4b-454b-a456-8c434f749f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import uvicorn\n",
    "# import nest_asyncio\n",
    "# import faiss\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from fastapi import FastAPI, HTTPException\n",
    "# from pydantic import BaseModel\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import ollama\n",
    "# from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "\n",
    "\n",
    "# # --- Initialize FastAPI ---\n",
    "# app = FastAPI()\n",
    "\n",
    "# # --- Load Faiss Index ---\n",
    "# index_path = \"modernbert_faiss_index.bin\"  \n",
    "# if os.path.exists(index_path):\n",
    "#     en_index = faiss.read_index(index_path)\n",
    "#     print(\"‚úÖ Faiss index loaded successfully!\")\n",
    "# else:\n",
    "#     raise FileNotFoundError(f\"‚ùå Index file not found at {index_path}\")\n",
    "\n",
    "# # --- Load Dataset (Metadata) ---\n",
    "# df = pd.read_csv(\"combined.csv\")\n",
    "\n",
    "# # --- Load ModernBERT Model ---\n",
    "# model_id = \"nomic-ai/modernbert-embed-base\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# modernbert_model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "# # --- Translation Function (Using Ollama Llama3.2) ---\n",
    "# MODEL_NAME = \"llama3.2:latest\"\n",
    "\n",
    "# def translate_to_english(text: str) -> str:\n",
    "#     \"\"\"Translates input text to English while preserving semantic meaning.\"\"\"\n",
    "#     prompt = f\"Convert the following text to English while preserving the semantic meaning, don't give an explanation:\\n\\n{text}\"\n",
    "#     response = ollama.chat(\n",
    "#         model=MODEL_NAME, \n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "#     )\n",
    "#     return response['message']['content'].strip()\n",
    "\n",
    "# # --- Embedding Function ---\n",
    "# def get_modernbert_embedding(text: str):\n",
    "#     \"\"\"Encodes a query using ModernBERT.\"\"\"\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = modernbert_model(**inputs)\n",
    "#     emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "#     return emb[0].astype(\"float32\")\n",
    "\n",
    "# # --- Semantic Search Function ---\n",
    "# def semantic_search(query_vector, k=5):\n",
    "#     \"\"\"Searches the Faiss index for the k nearest neighbors.\"\"\"\n",
    "#     query_vector = query_vector.reshape(1, -1)\n",
    "#     faiss.normalize_L2(query_vector)\n",
    "#     distances, indices = en_index.search(query_vector, k)\n",
    "    \n",
    "#     results = df.iloc[indices[0]][[\"S.No.\", \"Description\", \"Class\", \"Group\", \"Sub Class\"]].to_dict(orient=\"records\")\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # --- Define API Request Model ---\n",
    "# class QueryRequest(BaseModel):\n",
    "#     query: str\n",
    "#     k: int = 5\n",
    "\n",
    "# # --- Search Endpoint ---\n",
    "# @app.post(\"/search\")\n",
    "# async def search(request: QueryRequest):\n",
    "#     \"\"\"Performs a semantic search based on the user's input query.\"\"\"\n",
    "#     try:\n",
    "#         query_text = translate_to_english(request.query)  # Translate query to English\n",
    "#         query_vector = get_modernbert_embedding(query_text)  # Get embedding\n",
    "#         results = semantic_search(query_vector, request.k)  # Get top-k results\n",
    "        \n",
    "#         return {\"query\": request.query, \"translated_query\": query_text, \"results\": results}\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# # --- Root Endpoint ---\n",
    "# @app.get(\"/\")\n",
    "# async def root():\n",
    "#     return {\"message\": \"Welcome to the Semantic Search API!\"}\n",
    "# # Add CORS middleware to allow frontend to access the API\n",
    "# app.add_middleware(\n",
    "#     CORSMiddleware,\n",
    "#     allow_origins=[\"http://127.0.0.1:5500\"],  # Allow requests from your frontend\n",
    "#     allow_credentials=True,\n",
    "#     allow_methods=[\"*\"],  # Allow all HTTP methods (GET, POST, etc.)\n",
    "#     allow_headers=[\"*\"],  # Allow all headers\n",
    "# )\n",
    "\n",
    "\n",
    "# # --- Run FastAPI Inside Jupyter ---\n",
    "# nest_asyncio.apply()\n",
    "# uvicorn.run(app, host=\"0.0.0.0\", port=9050)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851212dc-a808-4fa1-b75d-5360cf1fabee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:‚úÖ Faiss index loaded successfully!\n",
      "ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-13' coro=<Server.serve() done, defined at C:\\Users\\avnex\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\server.py:68> exception=KeyboardInterrupt()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\avnex\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\main.py\", line 579, in run\n",
      "    server.run()\n",
      "    ~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\avnex\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\server.py\", line 66, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\avnex\\AppData\\Roaming\\Python\\Python313\\site-packages\\nest_asyncio.py\", line 30, in run\n",
      "    return loop.run_until_complete(task)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n",
      "  File \"C:\\Users\\avnex\\AppData\\Roaming\\Python\\Python313\\site-packages\\nest_asyncio.py\", line 92, in run_until_complete\n",
      "    self._run_once()\n",
      "    ~~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\avnex\\AppData\\Roaming\\Python\\Python313\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n",
      "    handle._run()\n",
      "    ~~~~~~~~~~~^^\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\events.py\", line 89, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\tasks.py\", line 386, in __wakeup\n",
      "    self.__step()\n",
      "    ~~~~~~~~~~~^^\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\tasks.py\", line 293, in __step\n",
      "    self.__step_run_and_handle_result(exc)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "  File \"c:\\Python313\\Lib\\asyncio\\tasks.py\", line 304, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "  File \"C:\\Users\\avnex\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\server.py\", line 69, in serve\n",
      "    with self.capture_signals():\n",
      "         ~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Python313\\Lib\\contextlib.py\", line 148, in __exit__\n",
      "    next(self.gen)\n",
      "    ~~~~^^^^^^^^^^\n",
      "  File \"C:\\Users\\avnex\\AppData\\Roaming\\Python\\Python313\\site-packages\\uvicorn\\server.py\", line 330, in capture_signals\n",
      "    signal.raise_signal(captured_signal)\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "INFO:     Started server process [7252]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:9060 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52730 - \"OPTIONS /search HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:üîÅ Sending prompt to Ollama\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:root:‚úÖ Received response from Ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52730 - \"POST /search HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:üîÅ Sending prompt to Ollama\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:root:‚úÖ Received response from Ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52743 - \"POST /search HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:üîÅ Sending prompt to Ollama\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:root:‚úÖ Received response from Ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52764 - \"POST /search HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:üîÅ Sending prompt to Ollama\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:root:‚úÖ Received response from Ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52782 - \"POST /search HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:üîÅ Sending prompt to Ollama\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:root:‚úÖ Received response from Ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52797 - \"POST /search HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import ollama\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "# --- Logging ---\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# --- Initialize FastAPI ---\n",
    "app = FastAPI()\n",
    "\n",
    "# --- Semaphore to throttle concurrent access to Ollama ---\n",
    "semaphore = asyncio.Semaphore(1)#update this part of the code based on your gpu specs test it multiple times\n",
    "\n",
    "# --- Load Faiss Index ---\n",
    "index_path = \"modernbert_faiss_index.bin\"\n",
    "if os.path.exists(index_path):\n",
    "    en_index = faiss.read_index(index_path)\n",
    "    logging.info(\"‚úÖ Faiss index loaded successfully!\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"‚ùå Index file not found at {index_path}\")\n",
    "\n",
    "# --- Load Dataset (Metadata) ---\n",
    "df = pd.read_csv(\"combined.csv\")\n",
    "\n",
    "# --- Load ModernBERT Model ---\n",
    "model_id = \"nomic-ai/modernbert-embed-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "modernbert_model = AutoModel.from_pretrained(model_id)\n",
    "\n",
    "# --- Translation Function (Using Ollama Llama3.2) ---\n",
    "MODEL_NAME = \"gemma3:4b\"\n",
    "\n",
    "def safe_ollama_call(prompt: str):\n",
    "    \"\"\"Function to call Ollama in a thread-safe way.\"\"\"\n",
    "    return ollama.chat(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "def translate_to_english(text: str) -> str:\n",
    "    \"\"\"Translates input text to English using Ollama with timeout and logging.\"\"\"\n",
    "    prompt = f\"Convert the following text to English while preserving the semantic meaning, don't give an explanation:\\n\\n{text}\"\n",
    "    logging.info(\"üîÅ Sending prompt to Ollama\")\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future = executor.submit(safe_ollama_call, prompt)\n",
    "        try:\n",
    "            response = future.result(timeout=20)  # 20-second timeout\n",
    "            logging.info(\"‚úÖ Received response from Ollama\")\n",
    "            return response['message']['content'].strip()\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            logging.error(\"‚è∞ Ollama timed out while translating\")\n",
    "            raise RuntimeError(\"Ollama timed out while translating.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"‚ùå Ollama error: {e}\")\n",
    "            raise RuntimeError(f\"Ollama error: {str(e)}\")\n",
    "\n",
    "# --- Embedding Function ---\n",
    "def get_modernbert_embedding(text: str):\n",
    "    \"\"\"Encodes a query using ModernBERT.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = modernbert_model(**inputs)\n",
    "    emb = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    return emb[0].astype(\"float32\")\n",
    "\n",
    "# --- Semantic Search Function ---\n",
    "def semantic_search(query_vector, k=5):\n",
    "    \"\"\"Searches the Faiss index for the k nearest neighbors.\"\"\"\n",
    "    query_vector = query_vector.reshape(1, -1)\n",
    "    faiss.normalize_L2(query_vector)\n",
    "    distances, indices = en_index.search(query_vector, k)\n",
    "\n",
    "    results = df.iloc[indices[0]][[\"S.No.\", \"Description\", \"Class\", \"Group\", \"Sub Class\"]].to_dict(orient=\"records\")\n",
    "    return results\n",
    "\n",
    "# --- Define API Request Model ---\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "    k: int = 5\n",
    "\n",
    "# --- Search Endpoint ---\n",
    "@app.post(\"/search\")\n",
    "async def search(request: QueryRequest):\n",
    "    \"\"\"Performs a semantic search based on the user's input query.\"\"\"\n",
    "    try:\n",
    "        async with semaphore:\n",
    "            query_text = translate_to_english(request.query)\n",
    "            query_vector = get_modernbert_embedding(query_text)\n",
    "            results = semantic_search(query_vector, request.k)\n",
    "            return {\"query\": request.query, \"translated_query\": query_text, \"results\": results}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# --- Root Endpoint ---\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Welcome to the Semantic Search API!\"}\n",
    "\n",
    "# --- Add CORS middleware ---\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"http://127.0.0.1:5500\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# --- Run FastAPI ---\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=9060)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
